<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" type="image/png" href="./images/g.jpg" />

  <title>
    Gokul Prasath Radhakrishnan | Data Engineer | Data & AI Enthusist
  </title>

  <meta name="description" content="A passionate Data Engineer with expertise in cloud computing, AI, and data solutions.">
  <meta name="keywords" content="Data Engineer, Azure, AI, Cloud Computing, Machine Learning, Data Solutions" />
  <link rel="stylesheet" href="style.css" />
</head>

<body>

  <!-- ***** Header ***** -->

  <header class="header" role="banner" id="top">
    <div class="row">
      <nav class="nav" role="navigation">
        <ul class="nav__items">
          <li class="nav__item"><a href="#experience" class="nav__link">Experience</a></li>
          <li class="nav__item"><a href="#project" class="nav__link">Project</a></li>
          <li class="nav__item"><a href="#education" class="nav__link">Education</a></li>
          <li class="nav__item"><a href="#certification" class="nav__link">Certification</a></li>
          <li class="nav__item"><a href="#about" class="nav__link">About</a></li>
          <li class="nav__item"><a href="#contact" class="nav__link">Contact</a></li>
        </ul>
      </nav>
    </div>
    <div class="header__text-box row">
      <div class="header__text">
        <h1 class="heading-primary">
          <span>Gokul Prasath Radhakrishnan</span>
        </h1>
        <p>Data Warehouse Engineer | Data Engineer | Data and AI Enthusiast</p>
        <a href="#contact" class="btn btn--pink">Get in touch</a>
      </div>
    </div>
  </header>

  <main role="main">
    <!-- ***** Experience ***** -->
    <section class="experience" id="experience">
      <div class="row">
        <h2>Professional Experience</h2>
        <div class="experience__boxes">
          <div class="experience__box">
            <div class="experience__text">
              <h3>Data Warehouse Engineer, Holland and Barrett, United Kingdom</h3>
              <ul>
                <li>Supported strategic data initiatives aligned with the companyâ€™s business roadmap.</li>
                <li>Collaborated with senior engineers on cross-functional projects</li>
                <li>Covered for senior engineers to maintain business-critical deliverables.</li>
                <li>Built knowledge of enterprise data warehouse models to support wider projects.</li>
                <li>Migrated Python UDFs to scalable solutions, improving performance. </li>
                <li>Contributed to data warehouse reliability enhancements and upgrades.</li>
              </ul>
              <br>
              <ul class="experience__list">
                <li><b>Skills:</b> Amazon Redshift, AWS S3, GitLab, Data Modelling, AWS Lambda,
                  Matillion, CI/CD Pipelines, Python</li>
              </ul>
              <br>
            </div>
          </div>

          <div class="experience__box">
            <div class="experience__text">
              <h3>Senior Data Engineer, Addend Analytics, India</h3>
              <ul>
                <li>Designed scalable and efficient ETL architecture</li>
                <li>Developed and managed CI/CD pipelines using Azure DevOps</li>
                <li>Optimized complex SQL queries for performance improvements</li>
                <li>Administered and maintained Azure databases</li>
                <li>Implemented Identity and Access Management (IAM) solutions</li>
                <li>Executed cloud cost optimization strategies to reduce expenditure</li>
              </ul>
              <br>
              <ul class="experience__list">
                <li><b>Skills:</b> Azure Synapse Analytics, Azure Cosmos DB, Azure DevOps, Azure Function App, Power BI, T-SQL, Data Modelling, Data Security and Governance,
                  Azure Administration, CI/CD Pipelines, Python</li>
              </ul>
              <br>
            </div>
          </div>

          <div class="experience__box">
            <div class="experience__text">
              <h3>Data Engineer, Addend Analytics, India</h3>
              <ul>
                <li>Built ETL pipelines to streamline data integration processes</li>
                <li>Authored stored procedures for data manipulation and transformation</li>
                <li>Executed data migrations and warehousing projects</li>
                <li>Developed basic Power BI reports with data transformations using Power Query, connecting to sources like Excel and SQL, and publishing interactive dashboards to Power BI Service.</li>
              </ul>
              <br>
              <ul class="experience__list">
                <li><b>Skills:</b> Azure Data Factory, Azure Logic Apps, Azure Data Lake, Azure Event Hub, PySpark, Python, Power BI, Google BigQuery, Relational DB Amdinistration</li>
              </ul>
              <br>
            </div>
          </div>

          <div class="experience__boxes">
          <div class="experience__box">
            <div class="experience__text">
              <h3>NLP and Chatbot Intern, bitWise Academy, USA(Remote)</h3>
              <p>
                Built a contextual AI tutoring chatbot (RASA) for primary students to support course-related tutoring. 
              </p>
              <ul class="experience__list">
                <li><b>Skills:</b> RASA, NLP, NLU, Python</li>
              </ul>
              <br>
            </div>
          </div>
            
          <div class="experience__boxes">
          <div class="experience__box">
            <div class="experience__text">
              <h3>Data Science Intern, Credo Systemz, India</h3>
              <p>
                Built a customer propensity engine to predict course enrolment likelihood. 
              </p>
              <ul class="experience__list">
                <li><b>Skills:</b> Matplotlib, Machine Learning, Predictive modelling, Scikit-learn, Python</li>
              </ul>
            </div>
          </div>
  
        </div>
      </div>
    </section>

          
    <!-- ***** Education ***** -->

    <section class="education" id="education">
      <div class="row">
        <h2>Educational Qualifications</h2>
        <div class="education__boxes">
          <div class="education__box">
            <div class="education__text">
              <h3>MSc Artificial Intelligence and Machine Learning</h3>
              <p>
                University of Birmingham 2023-2024 
              </p>
              <ul class="education__list">
                <li>Deep Learning</li>
                <li>Evolutionary Computation</li>
                <li>Computer Vision</li>
                <li>Intelligent Data Analysis</li>
              </ul>
              <br>
            </div>
          </div>

          <div class="education__box">
            <div class="education__text">
              <h3>MSc Data Science</h3>
              <p>
                Comibatore Institute of Technology 2016-2021 
              </p>
              <ul class="education__list">
                <li>Machine Learning</li>
                <li>Statistics</li>
                <li>Natural Language Processing</li>
                <li>Big Data (Hadoop)</li>
                <li>Relational Database Management System (RDBMS)</li>
                <li>Data Structures and Algorithms</li>
              </ul>
            </div>
          </div>

        </div>
      </div>
    </section>


          

    <!-- ***** Projects ***** -->

    <section class="project" id="project">
      <div class="row">
        <h2>Projects</h2>
        <div class="project__boxes">
          <div class="accordion">
    
            <div class="accordion-item">
              <h3 class="accordion-title">Interpretable Machine Learning for Customer Behaviour Prediction</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  This project focuses on developing and evaluating interpretable machine learning models to predict customer behavior in e-commerce, 
                  specifically determining whether a user session leads to a purchase. It introduces a novel interpretability evaluation framework that 
                  quantifies how understandable various algorithms are, aiming to strike a balance between model accuracy and interpretability.
                </p>
            
                <h4>Key Highlights</h4>
                <ul>
                  <li>Designed novel interpretability metrics using structural model features.</li>
                  <li>Compared performance vs. transparency across diverse ML algorithms.</li>
                  <li>Evaluated models using Time to Interpret, Interpretation Correctness, Model Complexity, and Subjective Ratings.</li>
                  <li>Built a regression model to score interpretability from model structure.</li>
                </ul>
                <br>
            
                <h4>Results</h4>
                <ul>
                  <li>Logistic Regression, Decision Tree, and Bayesian Decision Tree offered strong interpretability with good accuracy.</li>
                  <li>Random Forest and KAN delivered high AUROC but were harder to interpret.</li>
                  <li>KAN had high training time and needed data reduction for efficiency.</li>
                </ul>
                  <br>
            
                <h4>Skills & Tools</h4>
                <ul>
                  <li><strong>Languages & Frameworks:</strong> Python, Scikit-learn, TensorFlow</li>
                  <li><strong>Techniques:</strong> Supervised Learning, Feature Engineering, Interpretability Analysis</li>
                  <li><strong>Tools:</strong> Jupyter Notebook, Git, GitHub, Matplotlib, Pandas</li>
                </ul>
                <div class="project__links">
                  <a href="https://github.com/gokulprasathradhakrishnan/Interpretable_Machine_Learning_For_Customer_Behaviour_Prediction" class="btn">Visit Project</a>
                </div>
              </div>
            </div>

            <div class="accordion-item">
            <h3 class="accordion-title">Chat with SQL DB Using Streamlit and LangChain</h3>
            <button class="accordion-header">
              <span class="accordion-icon">+</span>
            </button>
            <div class="accordion-content">
              <h4>Overview</h4>
              <p>
                Built an interactive Streamlit application that allows users to dynamically query and interact with SQL databases using natural language.
                Leveraged LangChain's SQL agent capabilities powered by the LLaMA 3 model (via Groq API) to interpret user queries and return intelligent
                results. Integrated support for both local SQLite and external MySQL databases with dynamic connection inputs.
              </p>
          
              <h4>Key Highlights</h4>
              <ul>
                <li>Designed a dual-mode DB connector supporting SQLite (locally stored) and remote MySQL access with credential inputs.</li>
                <li>Implemented LLaMA3-based SQL agent using LangChainâ€™s `create_sql_agent` and Groqâ€™s high-performance LLM API.</li>
                <li>Built a responsive UI with Streamlit, including sidebar forms, real-time chat interface, and dynamic session state handling.</li>
                <li>Cached DB configuration using `@st.cache_resource` to avoid redundant loading and improve performance.</li>
                <li>Handled user messages, assistant responses, and stream callbacks for a real-time conversational experience.</li>
              </ul>
              <br>
              
              <h4>Results</h4>
              <ul>
                <li>Enabled natural language interaction with SQL databases without requiring technical SQL knowledge from end users.</li>
                <li>Achieved a smooth, responsive app experience thanks to caching and optimized message handling.</li>
                <li>Successfully demonstrated real-time query execution, error handling, and multi-database support in one unified interface.</li>
              </ul>
              <br>
          
              <h4>Skills & Tools</h4>
              <ul>
                <li>Python, Streamlit, SQLite, MySQL</li>
                <li>LangChain, ChatGroq, LLaMA3</li>
                <li>SQLDatabaseToolkit, SQLAlchemy</li>
                <li>StreamlitCallbackHandler, AgentType.ZERO_SHOT_REACT_DESCRIPTION</li>
                <li>Prompt Engineering, Natural Language Interfaces</li>
              </ul>
              <div class="project__links">
                  <a href="https://github.com/gokulprasathradhakrishnan/Chat_With_SQL" class="btn">Visit Project</a>
                </div>
            </div>
          </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">Object Detection Using Deep Learning Models</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  Designed and implemented end-to-end object detection pipelines using state-of-the-art deep learning models, including 
                  Faster R-CNN, SSD, and YOLOv3. Built scalable workflows for training, evaluation, and deployment using TensorFlow 2 
                  Object Detection API, both on local machines and cloud-based GPU environments. Integrated OpenCV for real-time inference 
                  and optimized the final models for production use.
                </p>
              
                <h4>Key Highlights</h4>
                <ul>
                  <li>Developed object detection pipelines from scratch using TensorFlow 2 with custom datasets.</li>
                  <li>Trained and evaluated models like Faster R-CNN, SSD, and YOLOv3 to detect multiple classes.</li>
                  <li>Implemented model freezing to generate lightweight, deployable versions of trained networks.</li>
                  <li>Integrated OpenCV to perform image-based object detection using exported models.</li>
                  <li>Configured and ran model training on Google Cloud AI Platform with GPU acceleration.</li>
                  <li>Monitored training progress using TensorBoard to track loss metrics and mean average precision (mAP).</li>
                </ul>
              <br>
                <h4>Results</h4>
                <ul>
                  <li>Achieved efficient and accurate object detection across diverse image sets using custom-trained models.</li>
                  <li>Reduced training time significantly by utilizing cloud GPUs, enabling faster experimentation cycles.</li>
                  <li>Final models were successfully deployed for inference, supporting real-time detection via OpenCV pipelines.</li>
                  <li>Improved model accuracy by tuning training parameters and evaluating mAP performance across classes.</li>
                </ul>
              <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>TensorFlow 2, Python, OpenCV</li>
                  <li>Faster R-CNN, SSD, YOLOv3</li>
                  <li>TensorFlow Object Detection API, TensorBoard</li>
                  <li>Model Freezing, mAP Evaluation, Real-time Inference</li>
                  <li>Google Cloud AI Platform (GPU-based training)</li>
                </ul>
              </div>
            </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">MRI Segmentation Using Traditional Methods</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  This project focuses on medical image segmentation of MRI brain slices using classical image processing techniques 
                  and K-Means clustering. The objective was to segment anatomical regions such as air, skin, skull, CSF, gray matter, 
                  and white matter and compare the results against ground truth labels. The work also extended to 3D segmentation 
                  using volumetric reconstruction and visualization.
                </p>
              
                <h4>Key Highlights</h4>
                <ul>
                  <li>Preprocessing included image normalization and noise removal using dilation and erosion.</li>
                  <li>Used Otsu thresholding, Canny edge detection, and connected components to extract region-specific masks (air, skin, skull).</li>
                  <li>Segmented brain matter (CSF, gray matter, white matter) using <code>cv2.kmeans</code> clustering.</li>
                  <li>Achieved pixel-wise segmentation by combining masks and comparing with ground truth for accuracy.</li>
                  <li>Extended analysis to 3D by stacking 2D slices and applying visualization via the Mayavi library.</li>
                  <li>Implemented additional K-Means segmentation across 3D volumes for unsupervised segmentation trials.</li>
                </ul>
              <br>
                <h4>Results</h4>
                <ul>
                  <li>2D segmentation approach achieved an accuracy of <strong>76%</strong> when validated against ground truth images.</li>
                  <li>3D segmentation was partially successful â€” air masks were accurate, but brain tissue regions showed low performance with K-Means (43% accuracy).</li>
                  <li>Limitations in 3D segmentation were due to spatial complexity and challenges with connected component labeling in volumetric data.</li>
                </ul>
              <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>Python, OpenCV, NumPy, Scikit-learn</li>
                  <li>Image Processing: Otsu Thresholding, Canny Edge Detection, Morphological Operations</li>
                  <li>Clustering: K-Means for segmentation of brain tissues</li>
                  <li>3D Visualization: Mayavi</li>
                  <li>Accuracy Evaluation: <code>sklearn.metrics.accuracy_score</code></li>
                </ul>
              </div>
            </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">Optimization Algorithms for TSP</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  This project tackles the classical Travelling Salesman Problem (TSP) using two advanced metaheuristic techniques: 
                  Simulated Annealing (SA) and Genetic Algorithm (GA). The goal was to find the shortest possible route visiting 
                  all cities once and returning to the origin. The implementation explores algorithmic strategies, parameter tuning, 
                  performance comparison, and visualization of optimization paths.
                </p>
                
                <h4>Key Highlights</h4>
                <ul>
                  <li>Implemented Simulated Annealing using the Metropolis criterion and 2-opt neighbor generation.</li>
                  <li>Built a Genetic Algorithm from scratch with permutation encoding, order crossover, and inverse mutation.</li>
                  <li>Used tournament selection and tuned hyperparameters for both algorithms (e.g., cooling rate, population size).</li>
                  <li>Compared performance based on average distance, standard deviation, and statistical significance testing.</li>
                  <li>Ran both algorithms across 30+ iterations to assess stability and variance in outcomes.</li>
                </ul>
                <br>
                <h4>Results</h4>
                <ul>
                  <li>Simulated Annealing achieved an average tour distance of <strong>36,578.79</strong> with a standard deviation of 1,119.92.</li>
                  <li>Genetic Algorithm achieved an average distance of <strong>around 40,000</strong>, but showed higher variability (SD: 1,642.53).</li>
                  <li>Statistical comparison using the Wilcoxon test indicated a significant difference between SA and GA results.</li>
                </ul>
                <br>
                <h4>Challenges & Learnings</h4>
                <ul>
                  <li>Balancing exploration and exploitation was tricky â€” required careful parameter tuning (especially mutation and cooling rate).</li>
                  <li>Understanding the impact of random initialization and convergence speed helped improve performance over multiple runs.</li>
                  <li>Fine-tuning parameters through random search led to better generalization across different city configurations.</li>
                  <li>Learned that smaller population sizes or poor mutation rates often caused premature convergence in GA.</li>
                </ul>
                <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>Python, NumPy, Matplotlib</li>
                  <li>Algorithm Design: Simulated Annealing, Genetic Algorithm</li>
                  <li>2-opt Optimization, Order Crossover, Inverse Mutation</li>
                  <li>Statistical Analysis: Wilcoxon Signed-Rank Test</li>
                  <li>Hyperparameter Tuning, Random Search</li>
                </ul>
                <div class="project__links">
                  <a href="https://github.com/gokulprasathradhakrishnan/Travelling_Salesman_Problem_Using_Simulated_Annealing_And_Genetic_Algorithm" class="btn">Visit Project</a>
                </div>
              </div>
            </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">Data Analysis using Principal Component Analysis</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  This project focuses on applying Principal Component Analysis (PCA) to a housing dataset obtained from Kaggle. 
                  The dataset includes various features related to residential properties. The primary objective was to reduce 
                  dimensionality while preserving variance, identify dominant data patterns, and uncover relationships between 
                  housing features like price and area per bedroom.
                </p>
                
                <h4>Key Highlights</h4>
                <ul>
                  <li>Explored and cleaned a dataset of 545 entries with 16 features.</li>
                  <li>Handled outliers using Winsorization to preserve data integrity.</li>
                  <li>Standardized the dataset before applying PCA for meaningful dimensionality reduction.</li>
                  <li>Performed labeling using price and areaperbedroom attributes with custom schemes (binning and quartiles).</li>
                  <li>Visualized explained variance, eigenvalues, and correlations with principal components.</li>
                </ul>
                <br>
                <h4>Results</h4>
                <ul>
                  <li>PCA reduced data to fewer dimensions while retaining over 90% variance with just 10 components.</li>
                  <li>Identified significant features contributing to variance: area, bathrooms, areaperbedroom.</li>
                  <li>Visualized 2D PCA plots showing clusters for price and areaperbedroom categories.</li>
                  <li>Detected multicollinearity using Variance Inflation Factor (VIF) and identified highly correlated variables.</li>
                </ul>
                <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>Python, Pandas, NumPy</li>
                  <li>Scikit-learn (PCA, StandardScaler)</li>
                  <li>Matplotlib, Seaborn for data visualization</li>
                  <li>Data preprocessing and outlier treatment using Winsorization</li>
                  <li>Dimensionality Reduction, Eigenvalue Analysis, VIF Calculation</li>
                </ul>
                <div class="project__links">
                  <a href="https://github.com/gokulprasathradhakrishnan/Analysis_of_House_Price_Using_Principal_Component_Analysis" class="btn">Visit Project</a>
                </div>
              </div>
            </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">Data Warehousing in Azure SQL</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  This project focused on building a scalable and reliable data warehousing solution in Azure SQL for financial data coming from 
                  Microsoft Business Central. The solution supports automated ETL for multiple countries, includes incremental data loading, 
                  and robust auditing with failure alerts to ensure high data integrity and observability.
                </p>
                
                <h4>Architecture & Flow</h4>
                <ul>
                  <li>Used Business Central API to programmatically extract financial data from multiple country-specific instances.</li>
                  <li>Created a metadata-driven pipeline using Azure Data Factory (ADF), referencing a control table that stores API endpoints and country codes.</li>
                  <li>Pipeline dynamically loops through the table, fetching data for each country in sequence.</li>
                  <li>Watermark table used to manage incremental data loads, ensuring only new/updated records are processed.</li>
                  <li>Error handling implemented using pipeline activity failure triggers with email notifications.</li>
                </ul>
                <br>
                <h4>Key Highlights</h4>
                <ul>
                  <li>Built fully dynamic and scalable ETL pipeline for multi-country data integration.</li>
                  <li>Implemented robust auditing using a custom audit table â€” tracks insert count, update count, errors, and timestamps.</li>
                  <li>Ensured 99.9% reliability with automated failure detection and alerting.</li>
                  <li>Created SQL views for seamless data consumption by downstream analysts and reporting tools.</li>
                </ul>
                <br>
                <h4>Results</h4>
                <ul>
                  <li>Delivered a reliable, maintainable, and scalable data pipeline architecture across multiple geographies.</li>
                  <li>Enabled business users to access unified and timely financial data for analytics and reporting.</li>
                  <li>Significantly reduced manual effort in data consolidation through automation and dynamic configurations.</li>
                </ul>
                <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>Azure Data Factory, Azure SQL Database</li>
                  <li>Microsoft Business Central API</li>
                  <li>Incremental Loading with Watermarking</li>
                  <li>Pipeline Parameterization & Dynamic Datasets</li>
                  <li>Auditing, Logging, and Error Notification Design</li>
                </ul>
              </div>
            </div>
    
            <div class="accordion-item">
              <h3 class="accordion-title">VSLA 2.0</h3>
              <button class="accordion-header">
                <span class="accordion-icon">+</span>
              </button>
              <div class="accordion-content">
                <h4>Overview</h4>
                <p>
                  VSLA 2.0 is an event-driven data processing solution built on Azure, designed to handle multilingual survey uploads 
                  and transform them into structured insights. The system automates data ingestion, transformation, translation, and auditing 
                  while supporting soft-deletes and dynamic schema evolution.
                </p>
              
                <h4>Architecture & Workflow Automation</h4>
                <ul>
                  <li>Survey files are uploaded through a web interface (built by the frontend team), where column names are pre-validated.</li>
                  <li>Uploaded XLSX files are stored in Azure Blob Storage and processed using an event-based trigger.</li>
                  <li>Python functions are triggered to convert Excel files to CSV and handle multi-file uploads.</li>
                  <li>Dynamic schema handling implemented using a staging pivot table to support variable column structures.</li>
                  <li>Column additions are managed using a control table with flags to dynamically evolve schema before inserting into fact and dimension tables.</li>
                  <li>Translation pipeline built using Azure Translator, with results stored in backend SQL tables for multilingual support on the frontend.</li>
                  <li>Soft delete strategy implemented using flags â€” latest record marked as active for SCD Type 2-like tracking.</li>
                  <li>CI/CD pipeline developed to promote resources across environments with high reliability.</li>
                </ul>
                <br>
                <h4>Key Highlights</h4>
                <ul>
                  <li>Event-driven design using Azure Data Lake Storage and Azure Functions for real-time ETL.</li>
                  <li>Dynamic XLS to CSV conversion with flexible column handling through a pivot staging model.</li>
                  <li>Full multilingual support with Azure Translator integration and dedicated translation pipeline.</li>
                  <li>Custom audit table tracking inserts, updates, error messages, and timestamps for full traceability.</li>
                  <li>Failure notification system ensures prompt error awareness across all pipeline activities.</li>
                </ul>
              <br>
                <h4>Results</h4>
                <ul>
                  <li>Achieved near-perfect reliability with resilient, automated ETL and robust auditing mechanisms.</li>
                  <li>Enabled multilingual survey data processing for global stakeholders, improving accessibility.</li>
                  <li>CI/CD automation streamlined the promotion of infrastructure and logic across multiple environments.</li>
                </ul>
              <br>
                <h4>Skills & Tools</h4>
                <ul>
                  <li>Azure Data Factory, Azure Blob Storage, Azure Data Lake</li>
                  <li>Python (XLS to CSV conversion, Azure Function)</li>
                  <li>Azure Translator API, Azure SQL Database</li>
                  <li>CI/CD Pipelines (Infrastructure Promotion)</li>
                  <li>SCD Type 2 Design Pattern, Dynamic Schema Handling</li>
                  <li>Auditing & Error Notification Systems</li>
                </ul>
              </div>
            </div>
    
          </div> 
        </div>
      </div>
    </section>

    <!-- ***** Certification ***** -->

    <section class="certification" id="certification">
      <div class="row">
        <h2>Courses and Certifications</h2>
        <div class="certification__boxes">
          <div class="certification__box">
            <div class="certification__text">
              <ul class="certification__list">
                <li>Microsoft: Azure Administration (AZ â€“ 104)
                  <div class="certification__links">
                <a href="https://learn.microsoft.com/en-us/users/gokul-8306/credentials/67dbce01992feb73?ref=https%3A%2F%2Fwww.linkedin.com%2F" class="link__text">
                  Visit Certificate <span>&rarr;</span>
                </a>
              </div>   
                </li>
                <li>Microsoft: Azure Relational Database Administration (DP â€“ 300)
                  <div class="certification__links">
                <a href="https://learn.microsoft.com/en-us/users/gokul-8306/credentials/1d2ff13823652d18?ref=https%3A%2F%2Fwww.linkedin.com%2F" class="link__text">
                  Visit Certificate <span>&rarr;</span>
                </a>
              </div>   
                </li>
                <li>BCG GenAI Job Simulation on Forage</li>
                <li>Azure Databricks & Data Engineering with Spark â€“ Udemy </li>
                <li>Microsoft Fabric - The Complete Guide - Udemy </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>
            


            
    <!-- ***** About ***** -->

    <section class="about" id="about">
      <div class="row">
        <h2>About Me</h2>
        <div class="about__content">
          <div class="about__text">
            <!-- Replace the below paragraph with info about yourself -->
            <p>
              I am currently working as a Data Warehouse Engineer at Holland & Barrett, UK, where I contribute to building and optimising enterprise data platforms. With more than two years of experience, I specialise in designing scalable data pipelines, automating workflows, and enhancing data reliability using Azure and AWS cloud technologies.
            </p>
            <p>
              I hold dual Masterâ€™s degrees: an MSc in Artificial Intelligence & Machine Learning from the University of Birmingham and an MSc in Data Science from Coimbatore Institute of Technology. This blend of academic depth and industry experience enables me to bridge data engineering and AI to deliver business value and data-driven innovation.
            </p>
              <a href="Gokul_Resume.pdf" class="btn">My Resume</a>
          </div>

          <div class="about__photo-container">
            <img class="about__photo" src="profile.jpg" alt="" />
          </div>
        </div>
      </div>
    </section>
  </main>

  <!-- ***** Contact ***** -->

  <section class="contact" id="contact">
    <div class="row">
      <h2>Get in Touch</h2>
      <div class="contact__info">
        <p>
          Whether you're seeking to build a robust, scalable data pipeline, want to dive into machine learning, 
          or need consultation in data engineering or AI/ML fields, I am here to help. Have any questions, feedback, 
          or just want to say "Hi ðŸ‘‹"? Feel free to reach out! I'm always open to new discussions and opportunities. 
          The quickest way to get in touch with me is via email, and I'll make sure to get back to you as soon as possible. ðŸ˜Š
        </p>
        <a href="mailto:gokulprasath1609@gmail" class="btn">Email</a>
      </div>
    </div>
  </section>

  <!-- ***** Footer ***** -->

  <footer role="contentinfo" class="footer">
    <div class="row">
      <ul class="footer__social-links">
        <li class="footer__social-link-item">
          <a href="https://github.com/gokulprasathradhakrishnan" title="Link to Github Profile">
            <img src="./images/github.svg" class="footer__social-image" alt="Github">
          </a>
        </li>
        <li class="footer__social-link-item">
          <a href=https://www.linkedin.com/in/gokul-prasath-radhakrishnan-63381015a>
            <img src="./images/linkedin.svg" title="Link to Linkedin Profile" class="footer__social-image" alt="Linkedin">
          </a>
        </li>
      </ul>

      <p>&copy; 2025 Gokul Prasath Radhakrishnan</p>
    </div>
  </footer>

  <a href="#top" class="back-to-top" title="Back to Top">
    <img src="./images/arrow-up.svg" alt="Back to Top" class="back-to-top__image"/>
  </a>
  <script src="./index.js"></script>
</body>

</html>
